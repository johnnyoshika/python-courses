{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd03e51c8fce8865fe7ce19d319b84dc281172c29fab73b40f06b05bf6ddc25df62",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3e51c8fce8865fe7ce19d319b84dc281172c29fab73b40f06b05bf6ddc25df62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Hypothesis tests covered in the course\n",
    "\n",
    "&nbsp; | Comparing a sample statistic to   a hypothesized population value for a… | Testing an association between a   binary categorical variable (2 variables) and a… | Testing an association between a   categorical variable with 3 or more categories and a…\n",
    "-- | -- | -- | --\n",
    "Quantitavie   Variable | One sample t-test | Two-sample t-test | ANOVA with Tukey's Range Test\n",
    "Categorical   Variable | Binomial Test | Chi-Square Test | Chi-Square Test\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## A/B Test\n",
    "\n",
    "Baseline Conversion Rate:\n",
    "e.g. our estimate for the percent of people who will buy a widget under the current website design (look at historical data to infer this), e.g. 16%\n",
    "Number can be written as a proportion (0.16) or percentage (16%).\n",
    "\n",
    "Minimum Detectable Effect (aka Desired Lift):\n",
    "The smallest difference that we actually care to measure, as a percent of the baseline conversion Rate\n",
    "e.g. Current conversion rate: 6%\n",
    "Desired conversion rate: 8%\n",
    "Minimum Detectable Effect: (8-6)/6 = 33%\n",
    "\n",
    "Significance Threshold:\n",
    "The threshold at which a change is considered significant. 0.05 (5%) is commonly used, which means that the \"null hypothesis\" will be rejected and \"B\" is significantly better than \"A\" if p-value is less than 0.05. 0.05 means the chances of a false positive is relatively low.\n",
    "The significance threshold turns out to be the false positive rate (i.e. the probability of finding a significant difference when there isn't one).\n",
    "There's a tradeoff between the false positive and false negative rates. Most A/B test sample size calculators estimate the sample size needed for a 20% false negative rate, while a data scientist chooses the false positive rate s/he is comfortable with. The lower the false positive rate, the larger the sample size needed.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Run a Chi Square calculation on an A/B Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Index Web_Version Purchased\n0      0           A        no\n1      1           A        no\n2      2           A       yes\n3      3           A       yes\n4      4           A       yes\nPurchased    no  yes\nWeb_Version         \nA            24   26\nB            15   35\n0.10096676200907678\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "data = pd.read_csv(\"a-b-test.csv\")\n",
    "print(data.head())\n",
    "\n",
    "# Calculate contingency table\n",
    "ab_contingency = pd.crosstab(data.Web_Version, data.Purchased)\n",
    "print(ab_contingency)\n",
    "\n",
    "# Run chi square test\n",
    "chi2, pval, dof, expected = chi2_contingency(ab_contingency)\n",
    "print(pval)\n",
    "\n",
    "# Based on this p-value, we would make a decision about which website design to use. A small p-value would provide evidence that the purchase rates are significantly different for the 2 groups, while a large p-value would suggest no significant difference."
   ]
  },
  {
   "source": [
    "## Simulating Data for a Chi-Square test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Website Purchased\n0  control       yes\n1  control        no\n2  control       yes\n3  control        no\n4  control       yes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulate 50% yes and 50% no for our control group (i.e. A test)\n",
    "sample_control = np.random.choice(['yes', 'no'], size=50, p=[.5, .5])\n",
    "# Simulate a lift of 30%\n",
    "sample_new_design = np.random.choice(['yes', 'no'], size=50, p=[.65, .35])\n",
    "\n",
    "# Assemble into a data frame\n",
    "\n",
    "# Create 100 rows, first 50 rows being 'control' and the next 50 being 'new_design'\n",
    "group = ['control']*50 + ['new_design']*50\n",
    "# Set outcome, with first 50 rows listing sample_control and next 50 sample_new_design\n",
    "outcome = list(sample_control) + list(sample_new_design)\n",
    "# Construct dictionary that looks like this: {'Website': ['control', 'control'], 'Purchased': ['no', 'yes']}\n",
    "sim_data = {\"Website\": group, \"Purchased\": outcome}\n",
    "# Convert to data frame\n",
    "sim_data = pd.DataFrame(sim_data)\n",
    "print(sim_data.head())"
   ]
  },
  {
   "source": [
    "### Generalize the previous script"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ab_contingency\ncol_0       no  yes\nrow_0              \ncontrol     23   27\nnew_design  23   27\nP Value: 1.0\nnot significant\n"
     ]
    }
   ],
   "source": [
    "# Run this a few times. Results will vary depending on random values selected by np.random.choice\n",
    "\n",
    "significance_threshold = 0.05\n",
    "sample_size = 100\n",
    "lift = .3\n",
    "control_rate = .5\n",
    "new_design_rate = (1 + lift) * control_rate\n",
    "\n",
    "# Simulate data\n",
    "\n",
    "sample_control = np.random.choice(['yes', 'no'], size=int(sample_size/2), p=[control_rate, 1-control_rate])\n",
    "sample_new_design = np.random.choice(['yes', 'no'], size=int(sample_size/2), p=[new_design_rate, 1-new_design_rate])\n",
    "\n",
    "group = ['control']*int(sample_size/2) + ['new_design']*int(sample_size/2)\n",
    "outcome = list(sample_control) + list(sample_new_design)\n",
    "sim_data = {\"Website\": group, \"Purchased\": outcome}\n",
    "sim_data = pd.DataFrame(sim_data)\n",
    "\n",
    "# run a chi-square test\n",
    "\n",
    "ab_contingency = pd.crosstab(np.array(sim_data.Website), np.array(sim_data.Purchased))\n",
    "print('ab_contingency')\n",
    "print(ab_contingency)\n",
    "chi2, pval, dof, expected = chi2_contingency(ab_contingency, correction=False)\n",
    "print(\"P Value:\", pval)\n",
    "\n",
    "result = ('significant' if pval < significance_threshold else 'not significant')\n",
    "print(result)"
   ]
  },
  {
   "source": [
    "### Run the previous script many times to simulate multiple outcomes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.23\n"
     ]
    }
   ],
   "source": [
    "# Here we're going to estimate the proportion of simuated datasets taht lead to a 'significant' result...\n",
    "\n",
    "def proportion_of_significant_results(significance_threshold, sample_size, lift):\n",
    "    control_rate = .5\n",
    "    new_design_rate = (1 + lift) * control_rate\n",
    "    simulations = 100\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(simulations):\n",
    "\n",
    "        # Simulate data\n",
    "\n",
    "        sample_control = np.random.choice(['yes', 'no'], size=int(sample_size/2), p=[control_rate, 1-control_rate])\n",
    "        sample_new_design = np.random.choice(['yes', 'no'], size=int(sample_size/2), p=[new_design_rate, 1-new_design_rate])\n",
    "\n",
    "        group = ['control']*int(sample_size/2) + ['new_design']*int(sample_size/2)\n",
    "        outcome = list(sample_control) + list(sample_new_design)\n",
    "        sim_data = {\"Website\": group, \"Purchased\": outcome}\n",
    "        sim_data = pd.DataFrame(sim_data)\n",
    "\n",
    "        # run a chi-square test\n",
    "\n",
    "        ab_contingency = pd.crosstab(np.array(sim_data.Website), np.array(sim_data.Purchased))\n",
    "\n",
    "        # Note: correction=False was removed from chi2_contingency().\n",
    "        # This does make a difference in the result, but not sure why Codeacademy did that.\n",
    "        chi2, pval, dof, expected = chi2_contingency(ab_contingency)\n",
    "\n",
    "        result = ('significant' if pval < significance_threshold else 'not significant')\n",
    "        results.append(result)\n",
    "\n",
    "    # proportion of significant results (aka power of the test):\n",
    "    results = np.array(results)\n",
    "    return np.sum(results == 'significant') / simulations\n",
    "\n",
    "print(proportion_of_significant_results(0.05, 100, .3))\n",
    "\n",
    "# Result is usually between 0.2 and 0.36. So 20% to 36% of the time, chi-square will report that there's a significant difference. That means that 64 to 80% of the time, we will conclude that there's not a significant difference, even though we're simulating an average lift of ~30%. The low significance_threshold sets a low false positive, but we're also getting many false negatives 64 to 80% of the time. We can increase the significant results (aka power of the test) by increasing the sample size and/or increasing the significance threshold. We'll do both of those later.\n"
   ]
  },
  {
   "source": [
    "### False positive\n",
    "\n",
    "Now let's determine the false positive by setting lift to 0. The result should be 'not significant', but let's see in what proportion 'significant' is reported."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.03\n"
     ]
    }
   ],
   "source": [
    "print(proportion_of_significant_results(0.05, 100, 0))\n",
    "\n",
    "# Result is about 0.05 (i.e. similar to significant_threshold). So ~5% of the time, we'll get a false positive."
   ]
  },
  {
   "source": [
    "### Power of the test\n",
    "\n",
    "The power of a test is the probability of correctly detecting a significant result (i.e. the proportion of detecting significant results when there really is one (i.e. when lift is > 0)). It's also called the true positive rate. Most sample size calculators aim for a power of 80%.\n",
    "\n",
    "Increasing the power of the test can be done in 2 ways:\n",
    "\n",
    "* Increasing the sample size increases the power of the test (the probability of detecting a difference if there is one); however, larger sample sizes require more time and resources.\n",
    "\n",
    "* Increasing the significance threshold also increases the power of the test; however, it simultaneously increases the false positive rate (the probability of detecting a difference when there isn’t one).\n",
    "\n",
    "Also, we can choose a larger minimum detectable effect/lift, which will decrease the sample size without decreasing power. However, setting a minimum lift of 30% (for example), we may not be able to detect smaller differences that are still meaningful."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Smaller lift\n",
    "\n",
    "Larger sample sizes are needed to detect smaller effect sizes (e.g. lift)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Increase sample size and significance threshold"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.87\n",
      "0.99\n",
      "0.11\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Increase the sample size to 500\n",
    "\n",
    "print(proportion_of_significant_results(0.05, 500, .3))\n",
    "\n",
    "# Result is about 0.9 now (i.e. we're reporting a significant result ~90% of the time with a simulated lift of 30%). We increase the power of the test, and got over the desired threshold of 80%.\n",
    "\n",
    "\n",
    "# Also increase the significant threshold to 0.10\n",
    "\n",
    "print(proportion_of_significant_results(0.10, 500, .3))\n",
    "\n",
    "# Result is even higher\n",
    "\n",
    "# Note that we also increased the false positive by increasing the significant result to 0.10\n",
    "\n",
    "print(proportion_of_significant_results(0.10, 500, 0))\n",
    "\n",
    "# Now increase the lift to 40%\n",
    "\n",
    "print(proportion_of_significant_results(0.10, 500, .4))\n",
    "\n",
    "# Wowza, now getting between 99% and 100%"
   ]
  }
 ]
}